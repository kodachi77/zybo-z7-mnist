{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINN - End-to-End Flow\n",
    "-----------------------------------------------------------------\n",
    "\n",
    "In this notebook, we will take a simple, binarized, fully-connected network trained on the MNIST data set and take it all the way down to a customized bitfile running on a Zybo Z7-20 board. \n",
    "\n",
    "This notebook is quite lengthy, and some of the cells (involving Vivado synthesis) may take up to an hour to finish running. To let you save and resume your progress, we will save the intermediate ONNX models that are generated in the various steps to disk, so that you can jump back directly to where you left off.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The FINN compiler comes with many *transformations* that modify the ONNX representation of the network according to certain patterns. This notebook demonstrates a sequence of such transformations to take a particular trained network all the way down to hardware, as shown in the figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](finn-design-flow-example.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The white fields show the state of the network representation in the respective step. The colored fields represent the transformations that are applied to the network to achieve a certain result. The diagram is divided into 5 sections represented by a different color, each of it includes several flow steps. The flow starts in top left corner with Brevitas export (green section), followed by the preparation of the network (blue section) to bring the network into a form in which each layer can be represented by either a Vitis HLS function or a Verilog module. The model then gets passed to Vivado IPI stitching (orange section), and finally a PYNQ overlay bitfile is built and can be tested on a PYNQ board (yellow section).\n",
    "There is an additional section for functional verification (red section) on the right side of the diagram, which we will not cover in this notebook. For details please take a look in the verification notebook which you can find [here](sfc_verification.ipynb)\n",
    "\n",
    "\n",
    "This Jupyter notebook is organized based on the sections described above. We will use the following helper functions, `showSrc` to show source code of FINN library calls and `showInNetron` to show the ONNX model at the current transformation step. The Netron displays are interactive, but they only work when running the notebook actively and not on GitHub (i.e. if you are viewing this on GitHub you'll only see blank squares)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import path: /home/kodachi77/project/finn/notebooks/training\n",
      "Build dir: /tmp/finn_dev_kodachi77\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from finn.util.visualization import showSrc, showInNetron\n",
    "from finn.util.basic import make_build_dir\n",
    "\n",
    "cwd = os.getcwd()\n",
    "import_path = os.path.join(cwd, '../training')\n",
    "import_path = os.path.abspath(import_path)\n",
    "sys.path.append(import_path)\n",
    "print(\"Import path: {0}\".format(import_path))\n",
    "    \n",
    "build_dir = os.environ.get(\"FINN_BUILD_DIR\", \"./build\")\n",
    "##build_dir = \"/home/kodachi77/project/finn/finn_dev_kodachi77\"\n",
    "#os.environ[\"FINN_BUILD_DIR\"] = build_dir\n",
    "print(\"Build dir: {0}\".format(build_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "-------------\n",
    "1. [Brevitas export](#brev_exp)\n",
    "2. [Network preparation](#nw_prep)\n",
    "3. [Hardware build](#vivado)\n",
    "4. [PYNQ deployment](#hw_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Brevitas export <a id='brev_exp'></a>\n",
    "FINN expects an ONNX model as input. This can be a model trained with [Brevitas](https://github.com/Xilinx/brevitas). Brevitas is a PyTorch library for quantization-aware training and the FINN Docker image comes with several [example Brevitas networks](https://github.com/Xilinx/brevitas/tree/master/src/brevitas_examples/bnn_pynq). To show the FINN end-to-end flow, we'll use the TFC-w1a1 model as example network.\n",
    "\n",
    "First a few things have to be imported. Then the model can be loaded with the pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kodachi77/project/finn/notebooks/training/models.py:96: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  package = torch.load(checkpoint, map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import onnx\n",
    "\n",
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "\n",
    "from models import fc\n",
    "\n",
    "sfc = fc()\n",
    "\n",
    "filename = os.path.join(import_path, 'checkpoints/sfc_1w1a.pth')\n",
    "assert os.path.isfile(filename)\n",
    "sfc.load_checkpoint(filename)\n",
    "\n",
    "export_onnx_path = os.path.join(build_dir, \"sfc_w1_a1.onnx\")\n",
    "export_qonnx(sfc, torch.randn(1, 1, 28, 28), os.path.join(build_dir, \"sfc_w1_a1.onnx\")); # semicolon added to suppress log\n",
    "qonnx_cleanup(export_onnx_path, out_file=export_onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was now exported in QONNX format, loaded with the pretrained weights and saved under the name \"tfc_w1_a1.onnx\".\n",
    "To visualize the exported model, Netron can be used. Netron is a visualizer for neural networks and allows interactive investigation of network properties. For example, you can click on the individual nodes and view the properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving '/tmp/finn_dev_kodachi77/sfc_w1_a1.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f6cdbec5810>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(build_dir+\"/sfc_w1_a1.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the model in .onnx format, we can work with it using FINN. For that, `ModelWrapper` is used. It is a wrapper around the ONNX model which provides several helper functions to make it easier to work with the model. `ModelWrapper` is imported from the [QONNX repo](https://github.com/fastmachinelearning/qonnx), this repository contains several functionality that is used in FINN. The model was exported in QONNX format, to feed it into the FINN flow, our first step is to convert it to the FINN-ONNX format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "model = ModelWrapper(build_dir+\"/sfc_w1_a1.onnx\")\n",
    "model = model.transform(ConvertQONNXtoFINN())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the conversion we save the model and visualize it using Netron. As you can see, quantization is now expressed differently. Where we had Quant nodes before, there are now MultiThreshold nodes present in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/tmp/finn_dev_kodachi77/sfc_w1_a1_finn.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f6c009c25c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save(build_dir+\"/sfc_w1_a1_finn.onnx\")\n",
    "showInNetron(build_dir+\"/sfc_w1_a1_finn.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model is prepared and could be simulated using Python. How this works is described in the Jupyter notebook about verification and can be found [here](tfc_end2end_verification.ipynb#simpy).\n",
    "\n",
    "The model can now also be processed in different ways. The principle of FINN are analysis and transformation passes, which can be applied to the model. An analysis pass extracts specific information about the model and returns it to the user in the form of a dictionary. A transformation pass changes the model and returns the changed model back to the FINN flow.\n",
    "\n",
    "Since the goal in this notebook is to process the model to such an extent that a bitstream can be generated from it, the focus is on the transformations that are necessary for this. In the next section these are discussed in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Network preparation <a id='nw_prep'></a>\n",
    "\n",
    "* [FINN-style Dataflow Architectures](#dataflow_arch)\n",
    "* [Tidy-up transformations](#basic_trafo)\n",
    "* [Streamlining](#streamline)\n",
    "* [Conversion to HW layers](#hw_layers)\n",
    "* [Creating a Dataflow Partition](#dataflow_partition)\n",
    "* [Specialize layers](#specialize_layers)\n",
    "* [Folding and Datawidth Converter, FIFO and TLastMarker Insertion](#folding)\n",
    "\n",
    "\n",
    "In this section, we will put the network through a series of transformations that puts it in a form that can be stitched together to form a FINN-style dataflow architecture, yielding a high-performance, high-efficiency FPGA accelerator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINN-style Dataflow Architectures <a id='dataflow_arch'></a>\n",
    "\n",
    "We start with a quick recap of FINN-style dataflow architectures. The key idea in such architectures is to parallelize across layers as well as within layers by dedicating a proportionate amount of compute resources to each layer, as illustrated in the figure below taken from the [FINN-R paper](https://arxiv.org/pdf/1809.04570.pdf):\n",
    "\n",
    "![](finn-hw-arch.png)\n",
    "\n",
    "In practice, the compute arrays are instantiated by function calls to optimized Vitis HLS building blocks from the [finn-hlslib](https://github.com/Xilinx/finn-hlslib) library or by Verilog modules from the [finn-rtllib](https://github.com/Xilinx/finn/tree/main/finn-rtllib). As these function calls/modules can only handle certain patterns/cases, we need to transform the network into an appropriate form so that we can replace network layers with these function calls/modules, which is the goal of the network preparation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidy-up transformations <a id='basic_trafo'></a>\n",
    "This section deals with some basic transformations, which are applied to the model like a kind of \"tidy-up\" to make it easier to be processed. They do not appear in the diagram above, but they are applied in many steps in the FINN flow to postprocess the model after a transformation and/or to prepare it for the next transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These transformations are:\n",
    "* GiveUniqueNodeNames\n",
    "* GiveReadableTensorNames\n",
    "* InferShapes\n",
    "* InferDataTypes\n",
    "* FoldConstants\n",
    "* RemoveStaticGraphInputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first two transformations (`GiveUniqueNodeNames`, `GiveReadableTensorNames`) the nodes in the graph are first given unique (by enumeration) names, then the tensors are given human-readable names (based on the node names). The following two transformations (`InferShapes`, `InferDataTypes`) derive the shapes and data types of the tensors from the model properties and set them in the `ValueInfo` of the model. These transformations can almost always be applied without negative effects and do not affect the structure of the graph, ensuring that all the information needed is available.\n",
    "\n",
    "The next listed transformation is `FoldConstants`, which performs constant folding. It identifies a node with constant inputs and determines its output. The result is then set as constant-only inputs for the following node and the old node is removed. Although this transformation changes the structure of the model, it is a transformation that is usually always desired and can be applied to any model. And finally, we have `RemoveStaticGraphInputs` to remove any top-level graph inputs that already have ONNX initializers associated with them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These transformations can be imported and applied as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "\n",
    "model.save(build_dir+\"/sfc_w1_a1_tidy.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of these transformations can be viewed with netron after the model has been saved again. By clicking on the individual nodes, it can now be seen, for example, that each node has been given a name. Also the whole upper area could be folded, so that now the first node is \"Reshape\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/tmp/finn_dev_kodachi77/sfc_w1_a1_tidy.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f6cdbec5780>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(build_dir+\"/sfc_w1_a1_tidy.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Pre- and Postprocessing <a id='prepost'></a>\n",
    "\n",
    "In many cases, it's common to apply some preprocessing to the raw data in a machine learning framework prior to training. For image classification networks, this may include conversion of raw 8-bit RGB values into floating point values between 0 and 1. Similarly, at the output of the network some postprocessing may be performed during deployment, such as extracting the indices of the classifications with the largest value (top-K indices).\n",
    "\n",
    "In FINN, we can bake some of these pre/postprocessing operatings into the graph, and in some cases these can be highly beneficial for performance by allowing our accelerator to directly consume raw data instead of going through CPU preprocessing. \n",
    "\n",
    "We'll demonstrate this for our small image classification network as follows. Brevitas preprocesses BNN-PYNQ network inputs with `torchvision.transforms.ToTensor()` [prior to training](https://github.com/Xilinx/brevitas/blob/master/src/brevitas_examples/bnn_pynq/trainer.py#L93), which converts 8-bit RGB values into floats between 0 and 1 by dividing the input by 255. We can achieve the same effect in FINN by exporting a single-node ONNX graph for division by 255 (which already exists as `finn.util.pytorch.ToTensor` and merging this with our original model. Finally, we're going to mark our input tensor as 8-bit to let FINN know which level of precision to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/tmp/finn_dev_kodachi77/sfc_w1_a1_with_preproc.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kodachi77/project/finn/deps/qonnx/src/qonnx/transformation/infer_data_layouts.py:127: UserWarning: Assuming 4D input is NCHW\n",
      "  warnings.warn(\"Assuming 4D input is NCHW\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f6c009c3f70>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.util.pytorch import ToTensor\n",
    "from qonnx.transformation.merge_onnx_models import MergeONNXModels\n",
    "from qonnx.core.datatype import DataType\n",
    "\n",
    "model = ModelWrapper(build_dir+\"/sfc_w1_a1_tidy.onnx\")\n",
    "global_inp_name = model.graph.input[0].name\n",
    "ishape = model.get_tensor_shape(global_inp_name)\n",
    "# preprocessing: torchvision's ToTensor divides uint8 inputs by 255\n",
    "totensor_pyt = ToTensor()\n",
    "chkpt_preproc_name = build_dir+\"/sfc_w1_a1_preproc.onnx\"\n",
    "export_qonnx(totensor_pyt, torch.randn(ishape), chkpt_preproc_name)\n",
    "qonnx_cleanup(chkpt_preproc_name, out_file=chkpt_preproc_name)\n",
    "pre_model = ModelWrapper(chkpt_preproc_name)\n",
    "pre_model = pre_model.transform(ConvertQONNXtoFINN())\n",
    "\n",
    "# join preprocessing and core model\n",
    "model = model.transform(MergeONNXModels(pre_model))\n",
    "# add input quantization annotation: UINT8 for all BNN-PYNQ models\n",
    "global_inp_name = model.graph.input[0].name\n",
    "model.set_tensor_datatype(global_inp_name, DataType[\"UINT8\"])\n",
    "\n",
    "model.save(build_dir+\"/sfc_w1_a1_with_preproc.onnx\")\n",
    "showInNetron(build_dir+\"/sfc_w1_a1_with_preproc.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe two changes in the graph above: a `Div` node has appeared in the beginning to perform the input preprocessing, and the `global_in` tensor now has a quantization annotation to mark it as an unsigned 8-bit value.\n",
    "\n",
    "For the postprocessing we'll insert a TopK node for k=1 at the end of our graph. This will extract the index (class number) for the largest-valued output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/tmp/finn_dev_kodachi77/sfc_w1_a1_pre_post.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f6c009c3340>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qonnx.transformation.insert_topk import InsertTopK\n",
    "\n",
    "# postprocessing: insert Top-1 node at the end\n",
    "model = model.transform(InsertTopK(k=1))\n",
    "chkpt_name = build_dir+\"/sfc_w1_a1_pre_post.onnx\"\n",
    "# tidy-up again\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "model.save(chkpt_name)\n",
    "\n",
    "showInNetron(build_dir+\"/sfc_w1_a1_pre_post.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the`TopK` node that has appeared at the end of the network. With our pre- and postprocessing in place, we can move on to the next step in the flow, which is streamlining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streamlining <a id='streamline'></a>\n",
    "Streamlining is a transformation containing several sub-transformations. The goal of streamlining is to eliminate floating point operations by moving them around, then collapsing them into one operation and in the last step transform them into multi-thresholding nodes. For more information on the theoretical background of this, see [this paper](https://arxiv.org/pdf/1709.04060).\n",
    "\n",
    "Let's have a look at which sub-transformations `Streamline` consists of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Streamline(Transformation):\n",
      "    \"\"\"Apply the streamlining transform, see arXiv:1709.04060.\"\"\"\n",
      "\n",
      "    def apply(self, model):\n",
      "        streamline_transformations = [\n",
      "            ConvertSubToAdd(),\n",
      "            ConvertDivToMul(),\n",
      "            BatchNormToAffine(),\n",
      "            ConvertSignToThres(),\n",
      "            MoveMulPastMaxPool(),\n",
      "            MoveScalarLinearPastInvariants(),\n",
      "            AbsorbSignBiasIntoMultiThreshold(),\n",
      "            MoveAddPastMul(),\n",
      "            MoveScalarAddPastMatMul(),\n",
      "            MoveAddPastConv(),\n",
      "            MoveScalarMulPastMatMul(),\n",
      "            MoveScalarMulPastConv(),\n",
      "            MoveAddPastMul(),\n",
      "            CollapseRepeatedAdd(),\n",
      "            CollapseRepeatedMul(),\n",
      "            MoveMulPastMaxPool(),\n",
      "            AbsorbAddIntoMultiThreshold(),\n",
      "            FactorOutMulSignMagnitude(),\n",
      "            AbsorbMulIntoMultiThreshold(),\n",
      "            Absorb1BitMulIntoMatMul(),\n",
      "            Absorb1BitMulIntoConv(),\n",
      "            RoundAndClipThresholds(),\n",
      "        ]\n",
      "        for trn in streamline_transformations:\n",
      "            model = model.transform(trn)\n",
      "            model = model.transform(RemoveIdentityOps())\n",
      "            model = model.transform(GiveUniqueNodeNames())\n",
      "            model = model.transform(GiveReadableTensorNames())\n",
      "            model = model.transform(InferDataTypes())\n",
      "        return (model, False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from finn.transformation.streamline import Streamline\n",
    "showSrc(Streamline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, several transformations are involved in the streamlining transformation. There are move and collapse transformations. In the last step the operations are transformed into multithresholds. The involved transformations can be viewed in detail [here](https://github.com/Xilinx/finn/tree/main/src/finn/transformation/streamline). After each transformation, three of the tidy-up transformations (`GiveUniqueNodeNames`, `GiveReadableTensorNames` and `InferDataTypes`) are applied to the model.\n",
    "\n",
    "After streamlining the network looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/tmp/finn_dev_kodachi77/sfc_w1_a1_streamlined.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f6cdbec5840>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.transformation.streamline.reorder import MoveScalarLinearPastInvariants\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "\n",
    "model = ModelWrapper(build_dir+\"/sfc_w1_a1_pre_post.onnx\")\n",
    "# move initial Mul (from preproc) past the Reshape\n",
    "model = model.transform(MoveScalarLinearPastInvariants())\n",
    "# streamline\n",
    "model = model.transform(Streamline())\n",
    "model.save(build_dir+\"/sfc_w1_a1_streamlined.onnx\")\n",
    "showInNetron(build_dir+\"/sfc_w1_a1_streamlined.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the network has become simplified considerably compared to the previous step -- a lot of nodes have disappeared between the `MatMul` layers. \n",
    "\n",
    "**The current implementation of streamlining is highly network-specific and may not work for your network if its topology is very different than the example network here. We hope to rectify this in future releases.**\n",
    "\n",
    "Our example network is a quantized network with 1-bit bipolar (-1, +1 values) precision, and we want FINN to implement them as XNOR-popcount operations [as described in the original FINN paper](https://arxiv.org/pdf/1612.07119). For this reason, after streamlining, the resulting bipolar matrix multiplications are converted into xnorpopcount operations. This transformation produces operations that are again collapsed and converted into thresholds. This procedure is shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/tmp/finn_dev_kodachi77/sfc_w1a1_ready_for_hw_conversion.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f6cdbec5a80>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qonnx.transformation.bipolar_to_xnor import ConvertBipolarMatMulToXnorPopcount\n",
    "from finn.transformation.streamline.round_thresholds import RoundAndClipThresholds\n",
    "from qonnx.transformation.infer_data_layouts import InferDataLayouts\n",
    "from qonnx.transformation.general import RemoveUnusedTensors\n",
    "\n",
    "model = model.transform(ConvertBipolarMatMulToXnorPopcount())\n",
    "model = model.transform(absorb.AbsorbAddIntoMultiThreshold())\n",
    "model = model.transform(absorb.AbsorbMulIntoMultiThreshold())\n",
    "# absorb final add-mul nodes into TopK\n",
    "model = model.transform(absorb.AbsorbScalarMulAddIntoTopK())\n",
    "model = model.transform(RoundAndClipThresholds())\n",
    "\n",
    "# bit of tidy-up\n",
    "model = model.transform(InferDataLayouts())\n",
    "model = model.transform(RemoveUnusedTensors())\n",
    "\n",
    "model.save(build_dir+\"/sfc_w1a1_ready_for_hw_conversion.onnx\")\n",
    "showInNetron(build_dir+\"/sfc_w1a1_ready_for_hw_conversion.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the pairs of `XnorPopcountmatMul` and `MultiThreshold` layers following each other -- this is the particular pattern that the next step will be looking for in order to convert them to hardware (HW) layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion to HW layers <a id='hw_layers'></a>\n",
    "Converts the nodes to HW layers, these layers are abstraction layers that do not directly correspond to an HLS or Verilog implementation but they will be converted in either one later in the flow. In our case this transformation converts pairs of binary XnorPopcountMatMul layers to MVAU layers (matrix vector activation unit). Any immediately following MultiThreshold layers will also be absorbed into the MVAU.\n",
    "\n",
    "Below is the code for the transformation and the network is visualized using netron to create the new structure with `MVAU` nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/tmp/finn_dev_kodachi77/sfc_w1_a1_hw_layers.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f6c009c32e0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import finn.transformation.fpgadataflow.convert_to_hw_layers as to_hw\n",
    "model = ModelWrapper(build_dir+\"/sfc_w1a1_ready_for_hw_conversion.onnx\")\n",
    "model = model.transform(to_hw.InferBinaryMatrixVectorActivation())\n",
    "# TopK to LabelSelect\n",
    "model = model.transform(to_hw.InferLabelSelectLayer())\n",
    "# input quantization (if any) to standalone thresholding\n",
    "model = model.transform(to_hw.InferThresholdingLayer())\n",
    "model.save(build_dir+\"/sfc_w1_a1_hw_layers.onnx\")\n",
    "showInNetron(build_dir+\"/sfc_w1_a1_hw_layers.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Dataflow Partition <a id='dataflow_partition'></a>\n",
    "\n",
    "In the graph above, you can see that there is a mixture of FINN HW layers (`MVAU` and `Thresholding`) with one regular ONNX layers (Reshape). To create a bitstream, FINN needs a model with only HW layers. In order to achieve this, we will use the `CreateDataflowPartition` transformation to create a \"dataflow partition\" in this graph, separating out the HLS layers into another model, and replacing them with a placeholder layer called StreamingDataflowPartition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/tmp/finn_dev_kodachi77/sfc_w1_a1_dataflow_parent.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f6cdbec5c00>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.transformation.fpgadataflow.create_dataflow_partition import CreateDataflowPartition\n",
    "\n",
    "model = ModelWrapper(build_dir+\"/sfc_w1_a1_hw_layers.onnx\")\n",
    "parent_model = model.transform(CreateDataflowPartition())\n",
    "parent_model.save(build_dir+\"/sfc_w1_a1_dataflow_parent.onnx\")\n",
    "showInNetron(build_dir+\"/sfc_w1_a1_dataflow_parent.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `MVAU` instances and the `Thresholding` in the beginning have all been replaced with a single `StreamingDataflowPartition`, which has an attribute `model` that points to the extracted, HW dataflow-only graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/tmp/finn_dev_kodachi77/dataflow_partition_5c2xgygu/partition_0.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f6bfe230880>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qonnx.custom_op.registry import getCustomOp\n",
    "sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "sdp_node = getCustomOp(sdp_node)\n",
    "dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "showInNetron(dataflow_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see all the extracted `MVAU` instances and the `Thresholding` have been moved to the child (dataflow) model. We will load the child model with `ModelWrapper` and continue working on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelWrapper(dataflow_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specialize layers <a id='specialize_layers'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is converted to HW abstraction layers and we have excluded the non-HW layers to continue with the processing of the model. HW abstraction layers are abstract (placeholder) layers that can be either implemented in HLS or as an RTL module using FINN. In the next flow step, we convert each of these layers to either an HLS or RTL variant by calling the `SpecializeLayers` transformation. It is possible to let the FINN flow know a preference for the implementation style `{\"hls\", \"rtl\"}` and depending on the layer type this wish will be fulfilled or it will be set to a reasonable default. In the tfc example, we will set all layers to their HLS variants. To showcase how to set the preferred implementation, we will set the node attribute in the `Thresholding` layer to `\"hls\"`, for the `MVAUs` and the `LabelSelect` we will leave this node attribute empty and in this case by default it will be set to HLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_node = model.get_nodes_by_op_type(\"Thresholding\")[0]\n",
    "thresh_node_inst = getCustomOp(thresh_node)\n",
    "thresh_node_inst.set_nodeattr(\"preferred_impl_style\", \"hls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define two helper variables that describe the Xilinx FPGA part name and the PYNQ board name that we are targeting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Ultra96', 'Ultra96-V2', 'Pynq-Z1', 'Pynq-Z2', 'Zybo-z7-20', 'ZCU102', 'ZCU104', 'ZCU111', 'RFSoC2x2', 'RFSoC4x2', 'KV260_SOM'])\n"
     ]
    }
   ],
   "source": [
    "# print the names of the supported PYNQ boards\n",
    "from finn.util.basic import pynq_part_map\n",
    "print(pynq_part_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this if you have a different PYNQ board, see list above\n",
    "pynq_board = \"Zybo-z7-20\"\n",
    "fpga_part = pynq_part_map[pynq_board]\n",
    "target_clk_ns = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will call `SpecializeLayers` to convert each HW abstraction layer to (in this case) an HLS variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/tmp/finn_dev_kodachi77/sfc_w1_a1_specialize_layers.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f6cc7772b90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.transformation.fpgadataflow.specialize_layers import SpecializeLayers\n",
    "model = model.transform(SpecializeLayers(fpga_part))\n",
    "\n",
    "model.save(build_dir+\"/sfc_w1_a1_specialize_layers.onnx\")\n",
    "showInNetron(build_dir+\"/sfc_w1_a1_specialize_layers.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each node type has now a suffix (`_hls`) and the module (`\n",
    "finn.custom_op.fpgadataflow.hls` also indicates that that the HLS variant of the layer is selected.\n",
    "We can now proceed by adjusting the parallelism of each node to customize the performance and resource usage.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folding: Adjusting the Parallelism <a id='folding'></a>\n",
    "\n",
    "*Folding* in FINN describes how much a layer is time-multiplexed in terms of execution resources. There are several *folding factors* for each layer, controlled by the PE (parallelization over outputs) and SIMD (parallelization over inputs) parameters as described by the original [FINN paper](https://arxiv.org/pdf/1612.07119). The higher the PE and SIMD values are set, the faster the generated accelerator will run, and the more FPGA resources it will consume. \n",
    "\n",
    "Each MVAU_hls node has two attributes that specify the degree of folding, PE and SIMD. In all nodes the values for these attributes are set as default to 1, which would correspond to a maximum folding (time multiplexing) and thus minimum performance. \n",
    "\n",
    "Since the folding parameters are node attributes, they can be easily accessed and changed using a helper function of the `ModelWrapper`. But first we take a closer look at one of the nodes that implement a Matrix-Vector-Activation operation. This is where the Netron visualization helps us, in the above diagram we can see that the model contains four `MVAUs`. So as an example we extract the second node of the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the higher-level CustomOp wrappers for this node. These wrappers provide easy access to specific properties of these nodes, such as the folding factors (PE and SIMD). Above, we have already used this abstraction to set the node attribute of the Thresholding HW layer.\n",
    "Let's have a look at which node attributes are defined by the CustomOp wrapper, and adjust the SIMD and PE attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomOp wrapper is of class MVAU_hls\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PE': ('i', True, 0),\n",
       " 'SIMD': ('i', True, 0),\n",
       " 'MW': ('i', True, 0),\n",
       " 'MH': ('i', True, 0),\n",
       " 'resType': ('s', False, 'auto', {'auto', 'dsp', 'lut'}),\n",
       " 'ActVal': ('i', False, 0),\n",
       " 'inputDataType': ('s', True, ''),\n",
       " 'weightDataType': ('s', True, ''),\n",
       " 'outputDataType': ('s', True, ''),\n",
       " 'accDataType': ('s', False, 'INT32'),\n",
       " 'binaryXnorMode': ('i', False, 0, {0, 1}),\n",
       " 'noActivation': ('i', False, 0, {0, 1}),\n",
       " 'numInputVectors': ('ints', False, [1]),\n",
       " 'mem_mode': ('s',\n",
       "  False,\n",
       "  'internal_decoupled',\n",
       "  {'external', 'internal_decoupled', 'internal_embedded'}),\n",
       " 'ram_style': ('s', False, 'auto', {'auto', 'block', 'distributed', 'ultra'}),\n",
       " 'ram_style_thresholds': ('s',\n",
       "  False,\n",
       "  'auto',\n",
       "  {'auto', 'block', 'distributed'}),\n",
       " 'runtime_writeable_weights': ('i', False, 0, {0, 1}),\n",
       " 'backend': ('s', True, 'fpgadataflow'),\n",
       " 'preferred_impl_style': ('s', False, '', {'', 'hls', 'rtl'}),\n",
       " 'code_gen_dir_ipgen': ('s', False, ''),\n",
       " 'ipgen_path': ('s', False, ''),\n",
       " 'ip_path': ('s', False, ''),\n",
       " 'ip_vlnv': ('s', False, ''),\n",
       " 'exec_mode': ('s', False, '', {'', 'cppsim', 'rtlsim'}),\n",
       " 'cycles_rtlsim': ('i', False, 0),\n",
       " 'cycles_estimate': ('i', False, 0),\n",
       " 'rtlsim_trace': ('s', False, ''),\n",
       " 'res_estimate': ('s', False, ''),\n",
       " 'res_synth': ('s', False, ''),\n",
       " 'rtlsim_so': ('s', False, ''),\n",
       " 'slr': ('i', False, -1),\n",
       " 'mem_port': ('s', False, ''),\n",
       " 'partition_id': ('i', False, 0),\n",
       " 'device_id': ('i', False, 0),\n",
       " 'inFIFODepths': ('ints', False, [2]),\n",
       " 'outFIFODepths': ('ints', False, [2]),\n",
       " 'output_hook': ('s', False, ''),\n",
       " 'io_chrc_in': ('t', False, array([], dtype=int32)),\n",
       " 'io_chrc_out': ('t', False, array([], dtype=int32)),\n",
       " 'io_chrc_period': ('i', False, 0),\n",
       " 'io_chrc_pads_in': ('ints', False, []),\n",
       " 'io_chrc_pads_out': ('ints', False, []),\n",
       " 'code_gen_dir_cppsim': ('s', False, ''),\n",
       " 'executable_path': ('s', False, ''),\n",
       " 'res_hls': ('s', False, '')}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc0 = model.graph.node[1]\n",
    "fc0w = getCustomOp(fc0)\n",
    "\n",
    "print(\"CustomOp wrapper is of class \" + fc0w.__class__.__name__)\n",
    "\n",
    "fc0w.get_nodeattr_types()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the PE and SIMD are listed as node attributes, as well as the depths of the FIFOs that will be inserted between consecutive layers, and all can be adjusted using `set_nodeattr` subject to certain constraints. There are also a lot of additional attributes that can be set for this node type.\n",
    "**In this notebook we are setting the folding factors and FIFO depths manually but it is possible to use FINN transformations for this ([SetFolding](https://github.com/Xilinx/finn/blob/main/src/finn/transformation/fpgadataflow/set_folding.py) and [InsertAndSetFIFODepths](https://github.com/Xilinx/finn/blob/main/src/finn/transformation/fpgadataflow/set_fifo_depths.py)).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layers = model.get_nodes_by_op_type(\"MVAU_hls\")\n",
    "# (PE, SIMD, in_fifo_depth, out_fifo_depth, ramstyle) for each layer\n",
    "config = [\n",
    "    (16, 49, [16], [64], \"block\"),\n",
    "    (16, 16, [64], [64], \"auto\"),\n",
    "    (16, 16, [64], [64], \"auto\"),\n",
    "    (10, 8, [64], [10], \"distributed\"),\n",
    "]\n",
    "for fcl, (pe, simd, ififo, ofifo, ramstyle) in zip(fc_layers, config):\n",
    "    fcl_inst = getCustomOp(fcl)\n",
    "    fcl_inst.set_nodeattr(\"PE\", pe)\n",
    "    fcl_inst.set_nodeattr(\"SIMD\", simd)\n",
    "    fcl_inst.set_nodeattr(\"inFIFODepths\", ififo)\n",
    "    fcl_inst.set_nodeattr(\"outFIFODepths\", ofifo)\n",
    "    fcl_inst.set_nodeattr(\"ram_style\", ramstyle)\n",
    "    \n",
    "# set parallelism for input quantizer to be same as first layer's SIMD\n",
    "inp_qnt_node = model.get_nodes_by_op_type(\"Thresholding_hls\")[0]\n",
    "inp_qnt = getCustomOp(inp_qnt_node)\n",
    "inp_qnt.set_nodeattr(\"PE\", 49)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are setting PE and SIMD so that each layer has a total folding of 16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides PE and SIMD three other node attributes are set. `ram_style` specifies how the weights are to be stored (BRAM, LUTRAM, and so on). It can be selected explicitly or with the option `auto` you can let Vivado decide.\n",
    "`inFIFODepths` and `outFIFODepths` specifies the FIFO depths that is needed by the node from the surrounding FIFOs. These attributes are used in the transformation 'InsertFIFO' to insert the appropriate FIFOs between the nodes, which will be automatically called as part of the hardware build process.\n",
    "\n",
    "In previous versions of FINN we had to call transformations to insert data width converters, FIFOs and `TLastMarker` manually at this step. This is no longer needed, as all this is taken care of by the `ZynqBuild` or `VitisBuild` transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/tmp/finn_dev_kodachi77/sfc_w1_a1_set_folding_factors.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f6cc7773cd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save(build_dir+\"/sfc_w1_a1_set_folding_factors.onnx\")\n",
    "showInNetron(build_dir+\"/sfc_w1_a1_set_folding_factors.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes the network preparation and the network can be passed on to the next block *Vitis HLS and IPI*, which is described below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hardware Build <a id='vivado'></a>\n",
    "\n",
    "We're finally ready to start generating hardware from our network. Depending on whether you want to target a Zynq or Alveo platform, FINN offers two transformations to build the accelerator, integrate into an appropriate shell and build a bitfile. These are `ZynqBuild` and `VitisBuild` for Zynq and Alveo, respectively. In this notebook we'll demonstrate the `ZynqBuild` as these boards are more common and it's much faster to complete bitfile generation for the smaller FPGAs found on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous versions of FINN, we had to manually go through several steps to generate HLS/RTL code, stitch IP, create a PYNQ project and run synthesis. All these steps are now performed by the `ZynqBuild` transform (or the `VitisBuild` transform for Alveo). **As this involves calling HLS synthesis and Vivado synthesis, this transformation will run for some time (up to half an hour depending on your PC).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kodachi77/project/finn/src/finn/transformation/fpgadataflow/floorplan.py:107: UserWarning: 10 nodes have no entry in the provided floorplan, SLR was set to -1\n",
      "  warnings.warn(\n",
      "/home/kodachi77/project/finn/src/finn/transformation/fpgadataflow/insert_fifo.py:234: UserWarning: Input FIFO for IODMA_hls_0_out0 has depth 2 and won't\n",
      "                        be created. This may cause RTL simulation issues.\n",
      "                        \n",
      "  warnings.warn(\n",
      "/home/kodachi77/project/finn/src/finn/transformation/fpgadataflow/insert_fifo.py:294: UserWarning: Output FIFO for LabelSelect_hls_0_out0 has depth 2 and won't\n",
      "                        be created. This may cause RTL simulation issues.\n",
      "                        \n",
      "  warnings.warn(\n",
      "/home/kodachi77/project/finn/src/finn/transformation/fpgadataflow/create_stitched_ip.py:290: UserWarning: First node is not StreamingFIFO or IODMA.\n",
      "                You may experience incorrect stitched-IP rtlsim or hardware\n",
      "                behavior. It is strongly recommended to insert FIFOs prior to\n",
      "                calling CreateStitchedIP.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from finn.transformation.fpgadataflow.make_zynq_proj import ZynqBuild\n",
    "model = ModelWrapper(build_dir+\"/sfc_w1_a1_set_folding_factors.onnx\")\n",
    "model = model.transform(ZynqBuild(platform = pynq_board, period_ns = target_clk_ns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the `ZynqBuild` we run one additional transformation to generate a PYNQ driver for the accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.fpgadataflow.make_pynq_driver import MakePYNQDriver\n",
    "model = model.transform(MakePYNQDriver(\"zynq-iodma\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(build_dir + \"/sfc_w1_a1_post_synthesis.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the generated outputs <a id='gen_outputs'></a>\n",
    "\n",
    "Let's start by viewing the post-synthesis model in Netron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/tmp/finn_dev_kodachi77/sfc_w1_a1_post_synthesis.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f6cdbec57b0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(build_dir + \"/sfc_w1_a1_post_synthesis.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our sequence of HLS layers has been replaced with `StreamingDataflowPartition`s, each of which point to a different ONNX file. You can open a Netron session for each of them to view their contents. Here, the first and last partitions contain only an `IODMA` node, which was inserted automatically to move data between DRAM and the accelerator. Let's take a closer look at the middle partition, which contains all our layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/tmp/finn_dev_kodachi77/dataflow_partition_ng5f7il0/partition_2.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f6cdbec6e00>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ModelWrapper(build_dir + \"/sfc_w1_a1_post_synthesis.onnx\")\n",
    "sdp_node_middle = getCustomOp(model.graph.node[1])\n",
    "postsynth_layers = sdp_node_middle.get_nodeattr(\"model\")\n",
    "\n",
    "showInNetron(postsynth_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `StreamingFIFO` and `StreamingDataWidthConverter` instances have been automatically inserted into the graph prior to hardware build. Both layer types are inserted as RTL variants. Transformations like `ZynqBuild` use the `metadata_props` of the model to put in additional metadata information relevant to the results of the transformation. Let's examine the metadata for the current graph containing all layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[key: \"floorplan_json\"\n",
       "value: \"/tmp/finn_dev_kodachi77/vitis_floorplan_jl1qdhed/floorplan.json\"\n",
       ", key: \"vivado_stitch_proj\"\n",
       "value: \"/tmp/finn_dev_kodachi77/vivado_stitch_proj_ncnyhi3r\"\n",
       ", key: \"clk_ns\"\n",
       "value: \"10\"\n",
       ", key: \"wrapper_filename\"\n",
       "value: \"/tmp/finn_dev_kodachi77/vivado_stitch_proj_ncnyhi3r/finn_vivado_stitch_proj.gen/sources_1/bd/StreamingDataflowPartition_1/hdl/StreamingDataflowPartition_1_wrapper.v\"\n",
       ", key: \"vivado_stitch_vlnv\"\n",
       "value: \"xilinx_finn:finn:StreamingDataflowPartition_1:1.0\"\n",
       ", key: \"vivado_stitch_ifnames\"\n",
       "value: \"{\\\"clk\\\": [\\\"ap_clk\\\"], \\\"rst\\\": [\\\"ap_rst_n\\\"], \\\"s_axis\\\": [[\\\"s_axis_0\\\", 392]], \\\"m_axis\\\": [[\\\"m_axis_0\\\", 8]], \\\"aximm\\\": [], \\\"axilite\\\": []}\"\n",
       ", key: \"platform\"\n",
       "value: \"zynq-iodma\"\n",
       "]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ModelWrapper(postsynth_layers)\n",
    "model.model.metadata_props"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that a Vivado project was built to create what we call the `stitched IP`, where all the IP blocks implementing various layers will be stitched together. You can view this stitched block design in Vivado, or [here](StreamingDataflowPartition_1.pdf) as an exported PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving back to the top-level model, recall that `ZynqBuild` will create a Vivado project and synthesize it, so it will be creating metadata entries related to the paths and files that were created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[key: \"floorplan_json\"\n",
       "value: \"/tmp/finn_dev_kodachi77/vitis_floorplan_jl1qdhed/floorplan.json\"\n",
       ", key: \"vivado_pynq_proj\"\n",
       "value: \"/tmp/finn_dev_kodachi77/vivado_zynq_proj_9nli3r_p\"\n",
       ", key: \"bitfile\"\n",
       "value: \"/tmp/finn_dev_kodachi77/vivado_zynq_proj_9nli3r_p/resizer.bit\"\n",
       ", key: \"hw_handoff\"\n",
       "value: \"/tmp/finn_dev_kodachi77/vivado_zynq_proj_9nli3r_p/resizer.hwh\"\n",
       ", key: \"vivado_synth_rpt\"\n",
       "value: \"/tmp/finn_dev_kodachi77/vivado_zynq_proj_9nli3r_p/synth_report.xml\"\n",
       ", key: \"platform\"\n",
       "value: \"zynq-iodma\"\n",
       ", key: \"pynq_driver_dir\"\n",
       "value: \"/tmp/finn_dev_kodachi77/pynq_driver_3x8k91hw\"\n",
       "]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ModelWrapper(build_dir + \"/sfc_w1_a1_post_synthesis.onnx\")\n",
    "model.model.metadata_props"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see the directories that were created for the PYNQ driver (`pynq_driver_dir`) and the Vivado synthesis project (`vivado_pynq_proj`), as well as the locations of the bitfile, hardware handoff file and synthesis report.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finn_zynq_link.cache\t      finn_zynq_link.runs  resizer.bit\t     vivado.jou\n",
      "finn_zynq_link.gen\t      finn_zynq_link.srcs  resizer.hwh\t     vivado.log\n",
      "finn_zynq_link.hw\t      finn_zynq_link.xpr   synth_project.sh\n",
      "finn_zynq_link.ip_user_files  ip_config.tcl\t   synth_report.xml\n"
     ]
    }
   ],
   "source": [
    "! ls {model.get_metadata_prop(\"vivado_pynq_proj\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to examine the generated Vivado project to get a feel for how the system-level integration is performed for the  FINN-generated \"stitched IP\", which appears as `StreamingDataflowPartition_1` in the top-level block design -- you can see it as a block diagram exported to PDF [here](top.pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  PYNQ deployment <a id='hw_test'></a>\n",
    "\n",
    "* [Deployment](#deploy)\n",
    "* [Validation on PYNQ Board](#validation)\n",
    "* [Throughput Test on PYNQ Board](#throughput)\n",
    "\n",
    "\n",
    "The bitfile and generated driver will be copied together with some necessary files for execution into a deployment folder which then can be used to run the network on the PYNQ board."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment <a id='deploy'></a>\n",
    "\n",
    "We'll now create a deployment folder with the bitfile and driver file(s), we zip it and afterwards it can be copied to the PYNQ board for execution and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/tmp/finn_dev_kodachi77/pynq_deployment_leutct02/driver_base.py',\n",
       " '/tmp/finn_dev_kodachi77/pynq_deployment_leutct02/finn/util/__init__.py',\n",
       " '/tmp/finn_dev_kodachi77/pynq_deployment_leutct02/finn/util/data_packing.py',\n",
       " '/tmp/finn_dev_kodachi77/pynq_deployment_leutct02/driver.py',\n",
       " '/tmp/finn_dev_kodachi77/pynq_deployment_leutct02/qonnx/core/datatype.py',\n",
       " '/tmp/finn_dev_kodachi77/pynq_deployment_leutct02/qonnx/core/__init__.py',\n",
       " '/tmp/finn_dev_kodachi77/pynq_deployment_leutct02/qonnx/util/__init__.py',\n",
       " '/tmp/finn_dev_kodachi77/pynq_deployment_leutct02/qonnx/util/basic.py',\n",
       " '/tmp/finn_dev_kodachi77/pynq_deployment_leutct02/validate.py']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from shutil import copy\n",
    "from distutils.dir_util import copy_tree\n",
    "\n",
    "# create directory for deployment files\n",
    "deployment_dir = make_build_dir(prefix=\"pynq_deployment_\")\n",
    "model.set_metadata_prop(\"pynq_deployment_dir\", deployment_dir)\n",
    "\n",
    "# get and copy necessary files\n",
    "# .bit and .hwh file\n",
    "bitfile = model.get_metadata_prop(\"bitfile\")\n",
    "hwh_file = model.get_metadata_prop(\"hw_handoff\")\n",
    "deploy_files = [bitfile, hwh_file]\n",
    "\n",
    "for dfile in deploy_files:\n",
    "    if dfile is not None:\n",
    "        copy(dfile, deployment_dir)\n",
    "\n",
    "# driver.py and python libraries\n",
    "pynq_driver_dir = model.get_metadata_prop(\"pynq_driver_dir\")\n",
    "copy_tree(pynq_driver_dir, deployment_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next to these files, we will also need an example numpy array to test the network on the PYNQ board. You may recall that one \"reshape\" node was left out of the StreamingDataflowPartition. We'll do that manually with a numpy function call when passing in the input, but everything else in the network ended up inside the StreamingDataflowPartition so that's all we need to do. The example numpy array can then be saved as .npy file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6c0763eef0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pkgutil import get_data\n",
    "import onnx.numpy_helper as nph\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "raw_i = get_data(\"qonnx.data\", \"onnx/mnist-conv/test_data_set_0/input_0.pb\")\n",
    "x = nph.to_array(onnx.load_tensor_from_string(raw_i))\n",
    "plt.imshow(x.reshape(28,28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected network input shape is [1, 784]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "model = ModelWrapper(build_dir + \"/sfc_w1_a1_post_synthesis.onnx\")\n",
    "iname = model.graph.input[0].name\n",
    "oname = parent_model.graph.output[0].name\n",
    "ishape = model.get_tensor_shape(iname)\n",
    "print(\"Expected network input shape is \" + str(ishape))\n",
    "np.save(deployment_dir + \"/input.npy\", x.reshape(ishape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "driver_base.py\tfinn\t   qonnx\tresizer.hwh\t validate.py\n",
      "driver.py\tinput.npy  resizer.bit\truntime_weights\n"
     ]
    }
   ],
   "source": [
    "! ls {deployment_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kodachi77/project/finn/notebooks/notebooks/deploy-on-pynq-sfc.zip'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from shutil import make_archive\n",
    "make_archive('deploy-on-pynq-sfc', 'zip', deployment_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now download the created zipfile (**File -> Open**, mark the checkbox next to the `deploy-on-pynq-sfc.zip` and select Download from the toolbar), then copy it to your PYNQ board (for instance via `scp` or `rsync`). Then, run the following commands **on the PYNQ board** to extract the archive and run the execution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "unzip deploy-on-pynq-sfc.zip -d finn-sfc-demo\n",
    "cd finn-sfc-demo\n",
    "sudo python3 -m pip install bitstring\n",
    "sudo python3 driver.py --exec_mode=execute --batchsize=1 --bitfile=resizer.bit --inputfile=input.npy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output will be saved on the PYNQ board as `output.npy` and can be copied to the host and opened with `np.load()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating the Accuracy on a PYNQ Board <a id='validation'></a>\n",
    "\n",
    "**Ensure that your PYNQ board has a working internet connecting for the next steps, since there is some downloading involved.**\n",
    "\n",
    "To validate the accuracy, we first need to install the [`dataset-loading`](https://github.com/fbcotter/dataset_loading) Python package to the PYNQ board. This will give us a convenient way of downloading and accessing the MNIST dataset.\n",
    "\n",
    "\n",
    "Command to execute on PYNQ board:\n",
    "\n",
    "```shell\n",
    "sudo pip3 install git+https://github.com/fbcotter/dataset_loading.git@0.0.4#egg=dataset_loading\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the `validate.py` script that was generated together with the driver to measure top-1 accuracy on the MNIST dataset.\n",
    "\n",
    "Command to execute on PYNQ board:\n",
    "\n",
    "```shell\n",
    "sudo python3 validate.py --dataset mnist --batchsize 1000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the final top-1 accuracy is 92.96%, which is very close to the 93.17% reported on the [BNN-PYNQ accuracy table in Brevitas](https://github.com/Xilinx/brevitas/tree/master/src/brevitas_examples/bnn_pynq). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Throughput Test on PYNQ Board <a id='throughput'></a>\n",
    "In addition to the functional verification, FINN also offers the possibility to measure the network performance directly on the PYNQ board. This can be done setting the `exec_mode` to `throughput_test`. \n",
    "Command to execute on PYNQ board:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "sudo python3 driver.py --exec_mode=throughput_test --batchsize=1000 --bitfile=resizer.bit\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network metrics from the throughput test are saved in a file called `nw_metrics.txt` on the PYNQ board. Which can be investigated after running the command above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
